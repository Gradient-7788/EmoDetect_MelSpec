# Speech Emotion Recognition model using audio mel-spectrograms

This work is done by Team Gradient as part of course project for DA221M Aug-Nov 2024 IIT Guwahati.
This project focuses on detecting emotions from speech using deep learning and audio preprocessing techniques like Mel-Spectrogram extraction. It combines exploratory analysis, model training, and result visualization for robust emotion classification.

---

## ğŸ“‚ Directory Structure

```bash
EmoDetect_MelSpec/
â”‚
â”œâ”€â”€ metrics/                        # Visual results and final evaluation
â”‚   â”œâ”€â”€ Final_metrics.png
â”‚   â””â”€â”€ Report_DS.pdf
â”‚
â”œâ”€â”€ notebook/                       # Jupyter notebook for training/experimentation
â”‚   â””â”€â”€ Speech_Emotion_Detection.ipynb
â”‚
â”œâ”€â”€ Dataset Links                   # Text file or markdown linking data sources
â”œâ”€â”€ README.md                       # Project overview and documentation
â”œâ”€â”€ Testing_model.jpg              # Model architecture or testing visualization
â”œâ”€â”€ app.py                          # Script for running predictions/inference
â””â”€â”€ requirements.txt
   ```

---

## Description 
This project focuses on developing a speech emotion recognition system using spectrograms and deep learning models, specifically EfficientNetB7 and ResNet, to classify emotions from audio data. The goal is to enhance human-robot interaction by accurately recognizing and understanding emotional states expressed through speech.

The division of methodology, and the corresponding location of code:
1. Dataset Preprocessing: ./PreprocessingInput/
2. Generation of spectrograms and integration of pipeline:
    1. For Multiclass classification task: ./FineTuneCNNs/Multiclass/ contains code of fine tuning of the various different models
    2. For Binary classification task: ./FineTuneCNNs/Binary/ contains code of fine tuning of the various different models
3. Audio Augmentation Techniques employed:
    1. For Data augmentation task: ... contains the code of various augmentation techniques employed
5. Training, Confusion Matrix and output generation:
    1. Generate predictions on all testing dataset created out of the overall dataframe.
    2. Encodings and Decodings (one hot) task: ... contains the overall total training, encodings and confusion matrix codes.

---

## ğŸ“Š Features

- ğŸ§ **Mel-Spectrogram Feature Extraction** for rich audio representation  
- ğŸ” **Preprocessing pipeline** including normalization and trimming  
- ğŸ§  **Deep learning-based model** trained on emotional speech samples  
- ğŸ“ˆ **Visualization** of performance metrics and confusion matrix  
- ğŸ§ª **Interactive notebook** for training, testing, and analysis
-  DFTs, FFTs,
-  Mel-Spectrograms,
-  Singular ValueÂ Decomposition
-  EfficientNetB7

---

## ğŸ› ï¸ Tech Stack

- Python 3.x  
- Librosa  
- NumPy, Pandas  
- Scikit-learn  
- Matplotlib, Seaborn  
- TensorFlow / Keras  
- Jupyter Notebook  

---

## ğŸ“Œ Getting Started

### 1. Clone the repository:

```bash
git clone https://github.com/Gradient-7788/EmoDetect_MelSpec.git
cd EmoDetect_MelSpec
  ```

### 2. Install Dependencies:

```bash
pip install -r requirements.txt
  ```

### 3. Run the notebook:

- Open notebook/Speech_Emotion_Detection.ipynb in Jupyter Notebook or JupyterLab
- Follow the notebook cells to preprocess audio, train, and evaluate the model

### 4. Run Inference:

```bash
python app.py
  ```

## ğŸ“œ License

MIT License. See LICENSE for details.

ğŸ‘¥ Contributor

Yesh Lohchab
yesh.3119@iitg.ac.in

